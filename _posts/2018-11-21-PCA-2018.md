---
layout:     post
title:      "深入理解PCA"
subtitle:   ""
date:       2018-11-30 15:55:00
author:     "pfzhang"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - Math
    - Computer Science
---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

# 前言

最近一周陆陆续续地看了一写关于SVD，PCA及其相关应用的文章和博客，在这个过程中，把之前很多其实没有搞懂的问题基本都搞清楚了，感觉很有必要写一篇总结性质的博客。PCA，SVD非常类似，但在细节处又有一些差别，需要我们认真去区分。以下我先会简单介绍这三种算法及其应用，最后来分析三者的相同点和不同点。 

这篇博客主要参考了以下这些资料：

- [Eigenfaces, for facial recognition](https://jeremykun.com/2011/07/27/eigenfaces/)
- [Singular Value Decomposition](https://jeremykun.com/2016/05/16/singular-value-decomposition-part-2-theorem-proof-algorithm/)
- [Neural Networks and Deep Learning: A Textbook](https://www.amazon.com/Neural-Networks-Deep-Learning-Textbook/dp/3319944622/ref=sr_1_4?ie=UTF8&qid=1543462897&sr=8-4&keywords=deep+learning)



# SVD

SVD全称是Singular Value Decomposition，中文名称是奇异值分解。 一个$$m$$x$$n$$实数矩阵A的奇异值分解为

$$A =U\Sigma V^{T}$$

其中U是一个$$m$$x$$m$$的正交矩阵(**矩阵的每一列的范数都为1，且两两正交**)，V是一个$$n$$x$$n$$的正交矩阵， $$\Sigma$$ 是一个$$m$$x$$m$$的对角矩阵，且其对角线上的值都为你非负。

其实我们可以这样看，我们可以将A看成是一个从n维向量空间到m维向量空间的线性变换。这里我们只处理实数向量，可以将n维向量空间表示为$$R^n$$, 把m维向量空间表示为$$R^m$$。而其矩阵分解的过程表达了$$R^n$$及$$R^m$$基变换的过程。具体来说， $$V$$表达了$$R^n$$的基从标准基变换到其他基的过程，$$U$$表达了$$R^m$$的基从标准基变换到其他基的过程。

虽然，可以将$$A$$看成是一个关于线性变换的矩阵，但是在大多数应用中，$$A$$常常包含了真实的数据，A的每一行都是一个n维的数据点，例如n个不同的用户给一部电影的打分情况，A中一共有m行，表示m个n维的数据点。这可以看成是A的另外一种解释。

现在我们需要具体说一下SVD的细节， 其中A是一个mxn的矩阵，它的秩为r(r $$\le$$min(m, n)。当对于任意的矩阵A进行SVD分解时，我们需要知道两组向量$$u$$'s和$$v$$'s。其中$$u$$'s是$$R^m$$中的向量，$$v's$$是$$R^n$$中的向量。他们组成mxm的矩阵U和nxn的矩阵V。

$$u$$'s和$$v$$'s给定4个基本子空间的基：

- $$u_1, ..., u_r$$是A的列空间(column space)的标准正交基 
- $$u_{r+1},..., u_{m}$$是A的左null空间(left nullspace) $$N(A^T)$$的标准正交基
- $$v_1, ..., v_r$$是A的行空间(row space)的标准正交基
- $$v_{r+1}, ..., v_n$$是A的null空间(nullspace)的标准正交基

而这些向量满足：

$$Av_1 = \sigma u_1$$, $$Av_2 = \sigma u_2$$, ..., $$Av_r = \sigma_r u_r$$ （1）

其中$$\sigma_1, \sigma_2,...,\sigma_r$$都是正实数，这样其实可以把$$\sigma_i$$看成是$$Av_i$$的长度。$$\sigma$$‘s会行成mxn的对角矩阵$$\Sigma$$的对角线的前r个值， 而$$\Sigma$$其他元素都为0。

根据式(1)及$$u$$'s和$$v$$'s的性质，我们可以得到：

$$AV = U\Sigma$$ （2）

又因为V中的列向量都为单位向量，且两两正交，所以$$V^{T}V= I$$,  $$V^T=V^{-1}$$。 所有式(2)可以转换为

$$A = U\Sigma V^T=u_1\sigma_1 v_1^T + ... + u_r\sigma_r v_r^T$$ (3)

其实，我们将会看到$$\sigma_i^2$$是$$A^TA$$和$$AA^T$$的特征值。 当我们令 $$\sigma_1 > \sigma_2 > \sigma_3> ... > \sigma_r$$时， 式(3)就可以看做是多个根据重要性排序的秩为1的矩阵$$u_i\sigma_i v_i^T$$相加的结果。

其实，我们可以将(3)改写为如下形式，

$$A = WG^T$$

其中$$W$$是一个mxr的矩阵，G是一个nxr矩阵，满足

$$W=[\sigma_1u_1, ..., \sigma_ru_r]$$, $$G=[v_1,...,v_r]$$

接下来我们将看到如何求解v's和u‘s。 首先我们知道

$$A^TA = (U\Sigma V^T)^T(U\Sigma V^T)=V\Sigma^TU^TU\Sigma V^T=V\Sigma^T\Sigma V^T$$ (4)

已知 $$A^TA$$是一个(半)正定矩阵，所以它必定可以分解成(4)所示的形式，此时V的每一列都是矩阵$$A^TA$$的特征向量，且这些向量都是单位向量，且两两正交，并且$$\Sigma^T\Sigma $$矩阵一定是$$A^TA$$的特征值矩阵。所以在求解V和$$\Sigma$$时，我们只需要对角化(半)正定矩阵$$A^TA$$即可。 而且已知 $$Av_i=\sigma_i u_i$$，我们可以很容易地求得$$u_1$$到$$u_r$$。 而SVD能够成功的最主要原因是对于任意 $$i \ne j$$:

$$u_i^Tu_j=(\frac{Av_i}{\sigma_i})^T(\frac{Av_j}{a_j}) = \frac{v_i^TA^TAv_j}{\sigma_i\sigma_j}$$

单位向量$$u_i$$与$$u_j$$正交，事实上这些u是$$AA^T$$的特征向量。



# PCA

PCA(**Principal components analysis**)，中文是主成分分析，PCA的问题其实是一个基的变换，使得数据在通过线性变换后，在前几维的特征上方差很大，而在其他维度上的方差大小可以忽略不计。方差的大小描述的是一个变量的信息量，我们在讲一个东西的稳定性的时候，往往说要减小方差，如果一个模型的方差很大，那就说明模型不稳定了。但是对于我们用于机器学习的数据（主要是训练数据），方差大才有意义，不然输入的数据都是同一个点，那方差就为0了，这样输入的多个数据就等同于一个数据了。所以，通过这样的一个基变换，我们可以只取前几个方差比较大的特征(称为组成分)，而可以舍弃其他特征。

现在假设一个mxn的矩阵M中存储了m个点，每个点都有n个特征(即矩阵的每一行存储了一个数据点)。我们需要对这n个点进行主成分分析，一开始我们需要对M的每一列进行求平均值得到$$\mu_1, ..., \mu_n$$，然后M中每一列的值都减去该列的平均值得到矩阵A，使得A每一列的平均值都为0。这样，这m个中心就是原点**0**了。

之后我们需要求解A的协方差矩阵C，容易求证：

$$C=\frac{A^TA}{n-1}$$

其中$$C_{11}$$表示点集在第一维上的方差，$$C_{12}$$表示第一维特征和第二维特征的协方差。已知C是(半)正定矩阵，所以我们可以对该矩阵进行对角化操作。

$$C = Q\Delta Q^T$$

$$\Delta$$是一个nxn的对角矩阵， 且对角线上的值降序排列，$$Q$$是一个nxn的单位正交方阵。我们假设Q中的列为$$q_1, q_2, q_3,...,q_n$$, 这些单位正交向量构成了$$R^n$$的一组标准正交基，且在$$q_1$$方向上数据的方差是最大的，$$q_1$$就是这些数据的第一个正成分，之后是$$q_2$$, $$q_3$$...

显而易见，上述过程正是我们对矩阵A就行SVD分解的过程，其中的矩阵Q对应了SVD中矩阵V，矩阵$$\Delta$$对应了SVD中的$$\Sigma^T\Sigma$$（$$\Delta$$可能需要乘以n-1才是我们想要的$$\Sigma^T\Sigma$$）。但得到$$\Sigma$$和V之后，我们很容易就可以得到矩阵U。

由此可见，我们也可以从式(3)的角度来解释PCA，当$$\sigma_i$$满足$$\sigma_1\gg \sigma_2 \gg ...\gg \sigma_r$$时，显而易见，排在前面的$$\sigma u_iv_i^T$$对A的贡献比较大, 所以我们可以用前几个$$\sigma u_i v_i^T$$的和来近似A，而舍去排在后面$$\sigma u_i v_i^T$$。



最后，还需要搞清楚一个问题，那就是为什么PCA有mean-centered的要求，这个问题曾经困扰了我很长时间。

主要原因其实是大家都习惯了用SVD来解决PCA问题，即通过下面这个流程：

Data **X** $$\to$$ Matrix $$X^TX/(n-1)$$ $$\to$$ 对角化操作。

而这个流程只有当数据X是mean-centered的时候是正确的，因为只有这样， $$X^TX/(n-1)$$才和X的协方差矩阵相同。 如果X不是mean-centered，那么这个过程其实只是在求X的SVD分解，而并非求X的PCA。同时，我们也可以这样说只有当X时mean-centered时候，那么求X的SVD和求其PCA是完全相同的。

